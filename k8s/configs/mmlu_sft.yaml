project:
  name: "mmlu"

dataset:
  type: "s3"
  name: "fine-tuning-research/mmlu"
  format: "completion"

model:
  base:
    type: "hf"
    name: "meta-llama/Llama-2-7b-hf"
  output:
    type: "hf"
    name: "rparundekar/llama2-7b-mmlu"

training:
  trainer:
    packing: False
    max_seq_length: 512
  sft:
    per_device_train_batch_size: 24
    per_device_eval_batch_size: 24
    bf16: True
    learning_rate: 0.0002
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1
    max_steps: 500
    optim: "paged_adamw_32bit"
    max_grad_norm: 0.3
    gradient_accumulation_steps: 4
    gradient_checkpointing: True
    gradient_checkpointing_kwargs:
      use_reentrant: False
    logging_strategy: "steps"
    logging_steps: 5
    evaluation_strategy: "steps"
    eval_steps: 100
  other:
    freeze_embed: True
    n_freeze: 24
